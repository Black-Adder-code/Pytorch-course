{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se_03.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE03_CA_Training_neural_networks.ipynb)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Look for hints if you get stuck\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/success.svg\" width=\"20\" /> Compare your solution with the provided answers\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt -O colab_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=3 -i colab_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "\n",
    "checker = utils.core.ExerciseChecker(\"SE03\")\n",
    "quizzer = utils.core.QuizManager(\"SE03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PyTorch workflow\n",
    "***\n",
    "The previous session we had a look at the basics of neural networks and how to train a single layer perceptron. In this session we will look at the PyTorch framework and how to use it to build and train neural networks.\n",
    "\n",
    "Most deep learning projects follow a similar workflow. The following figure illustrates the typical workflow of a PyTorch project:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/pytorch_workflow.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "\n",
    "| Step | Description |\n",
    "|------|----------|\n",
    "| Obtain Data | Collect and preprocess the data for training and testing |\n",
    "| Prepare Data | Setup data in PyTorch format |\n",
    "| Pre-process Data | Normalize and augment the data. This may involve data cleaning, normalization, and splitting the data into training, validation, and test sets. |\n",
    "| Activation Function | Choose an activation function for the model. This may involve selecting a suitable activation function for the model, such as ReLU, sigmoid, or tanh. |\n",
    "| Model | Define the model architecture. |\n",
    "| Choose optimiser | Select an optimiser for the model. |\n",
    "| Choose loss function | Select a loss function for the model. |\n",
    "| Create training loop | Define the training steps, including forward pass, backward pass, and parameter updates. |\n",
    "| Fit model | Train the model using the training data. |\n",
    "| Evaluate model | Evaluate the model using the validation and test data to make predictions |\n",
    "| Improve model | Fine-tune the model by adjusting hyperparameters, adding regularization, or modifying the architecture. |\n",
    "| Save or deploy model | Save the trained model for future use or deploy it in a production environment. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Obtain Data\n",
    "***\n",
    "In this notebook we are going to be using the [ARKOMA dataset](https://www.sciencedirect.com/science/article/pii/S2352340923007989). The dataset is intended to be used as a benchmark for the creation of Neural Networks to perform inverse kinematics for robotic arms using a NAO robot. The dataset contains data for two different robotic arms: the left arm and the right arm. The data is generated using a physics engine that simulates the movement of the robotic arms in a 3D environment. The dataset contain 10,000 input-output data pairs for both arms. The input data is the end-effector position of the robotic arm, and the output data is the joint angles of the robotic arm. \n",
    "\n",
    "The input parameters are:\n",
    "\n",
    "| Notation | Description |\n",
    "|------|----------|\n",
    "| $ P_{x} $ | The end-effector position with respect to the torso's x-axis |\n",
    "| $ P_{y} $ | The end-effector position with respect to the torso's y-axis |\n",
    "| $ P_{z} $ | The end-effector position with respect to the torso's z-axis |\n",
    "| $ R_{x} $ | The end-effector orientation relative to the torso's x-axis |\n",
    "| $ R_{y} $ | The end-effector orientation relative to the torso's y-axis |\n",
    "| $ R_{z} $ | The end-effector orientation relative to the torso's z-axis |\n",
    "\n",
    "The output parameters are:\n",
    "\n",
    "| Notation | Left Arm Joint | Left Arm Range(rad) | Right Arm Joint | Right Arm Range(rad) |\n",
    "|----------|----------------|--------------------|-----------------|--------------------|\n",
    "| $ \\theta_{1} $ | LShoulder Pitch | [-2.0857, 2.0857] | RShoulder Pitch | [-2.0857, 2.0857] |\n",
    "| $ \\theta_{2} $ | LShoulder Roll | [-0.3142, 1.3265] | RShoulder Roll | [-1.3265, 0.3142] |\n",
    "| $ \\theta_{3} $ | LElbow Yaw | [-2.0857, 2.0857] | RElbow Yaw | [-2.0857, 2.0857] |\n",
    "| $ \\theta_{4} $ | LElbow Roll | [-1.5446, 0.0349] | RElbow Roll | [-0.0349, 1.5446] |\n",
    "| $ \\theta_{5} $ | LWrist Yaw | [-1.8238, 1.8238] | RWrist Yaw | [-1.8238, 1.8238]  |\n",
    "\n",
    "In this notebook, we are going to focus on the right arm. The data is stored in CSV format. To load the data, we will use the `pandas` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('ARKOMA',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the datasets (already provided above)\n",
    "right_arm_path = dataset_path / 'Right Arm Dataset'\n",
    "\n",
    "# Create file paths using a dictionary comprehension and format strings\n",
    "file_parts = ['Train', 'Val', 'Test']\n",
    "dataset_files = {\n",
    "    part: {\n",
    "        'features': right_arm_path / f'R{part}_x.csv',\n",
    "        'targets': right_arm_path / f'R{part}_y.csv'\n",
    "    } for part in file_parts\n",
    "}\n",
    "\n",
    "# Unpack into individual variables for compatibility with existing code\n",
    "feats_train = dataset_files['Train']['features']\n",
    "targets_train = dataset_files['Train']['targets']\n",
    "feats_val = dataset_files['Val']['features']\n",
    "targets_val = dataset_files['Val']['targets']\n",
    "feats_test = dataset_files['Test']['features']\n",
    "targets_test = dataset_files['Test']['targets']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 and 3: Prepare and Pre-process Data\n",
    "***\n",
    "The next step is to pre-process the data. This involves normalizing the data and splitting it into training, validation, and test sets.\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "***\n",
    "One of the crucial steps in machine learning is to split the data into training, validation, and test sets. Each of these sets serves a specific purpose in the model development process:\n",
    "\n",
    "| Dataset | Purpose | Typical Split | Usage | Analogy |\n",
    "|---------|---------|---------------|--------|----------|\n",
    "| Training Set | Used to train the model by adjusting weights and biases through backpropagation | 60-80% | Every training iteration | Like studying materials to learn a subject |\n",
    "| Validation Set | Used to tune hyperparameters and monitor model performance during training to prevent overfitting | 10-20% | During model development | Like practice exams to gauge learning progress |\n",
    "| Test Set | Used only once for final model evaluation; never used for training or tuning | 10-20% | Once, after training | Like a final exam with new, unseen questions |\n",
    "\n",
    "The ARKOMA dataset has already been split into these three sets for us, which simplifies our workflow.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/>  **Note**: The test set is our generalisation benchmark. It is important to keep the test set separate from the training and validation sets to ensure that the model's performance is evaluated on unseen data. This helps us understand how well the model will perform in real-world scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "# Training set\n",
    "X_train = pd.read_csv(feats_train)\n",
    "y_train = pd.read_csv(targets_train)\n",
    "# Test set\n",
    "X_test = pd.read_csv(feats_test)\n",
    "y_test = pd.read_csv(targets_test)\n",
    "# Validation set\n",
    "X_val = pd.read_csv(feats_val)\n",
    "y_val = pd.read_csv(targets_val)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape} | y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape} | y_test shape: {y_test.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape} | y_val shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "***\n",
    "Normalisation is a crucial step in the pre-processing of data for machine learning models. It involves scaling the input features to a similar range, which helps improve the convergence speed and performance of the model. In this notebook, we will use Min-Max normalization to scale the input features to a range of [0, 1]. The formula for Min-Max normalization is as follows:\n",
    "$$ X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
    "\n",
    "Where:\n",
    "- $ X_{norm} $ is the normalized value.\n",
    "- $ X$ is the original value.\n",
    "- $ X_{min} $ is the minimum value of the feature.\n",
    "- $ X_{max} $ is the maximum value of the feature.\n",
    "\n",
    "The normalisation parameters will be computed from the training set and then applied to the validation and test sets. This helps to prevent data leakage and ensures that the model is evaluated on unseen data. \n",
    "\n",
    "| Benefit | Description | Impact on Training |\n",
    "|---------|-------------|-------------------|\n",
    "| **Faster Convergence** | Normalized inputs lead to better-conditioned optimization | Reduces training time |\n",
    "| **Numerical Stability** | Prevents extremely large or small values | Reduces risk of gradient explosions/vanishing |\n",
    "| **Feature Scaling** | Makes all features contribute equally to the model | Prevents certain features from dominating |\n",
    "| **Better Generalization** | Helps models transfer between different images | Improves performance on unseen data |\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Normalisation using Min-Max scaling\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the training\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Inverse transform the scaled data to get the original values\n",
    "X_train_original = scaler.inverse_transform(X_train_scaled)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Data Loading and Preprocessing 🎯\n",
    "\n",
    "# Create PyTorch tensors from the training, validation, and test data\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)  \n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create MinMaxScalers for feature and target normalization\n",
    "x_scaler = # Your code here\n",
    "y_scaler = # Your code here\n",
    "\n",
    "# Fit the scalers on training data \n",
    "x_scaler = # Your code here\n",
    "y_scaler = # Your code here\n",
    "\n",
    "\n",
    "# Transform all datasets and put them into tensors\n",
    "X_train_scaled = # Your code here\n",
    "X_val_scaled = # Your code here\n",
    "X_test_scaled = # Your code here\n",
    "\n",
    "\n",
    "y_train_scaled = # Your code here\n",
    "y_val_scaled = # Your code here\n",
    "y_test_scaled = # Your code here\n",
    "\n",
    "# Check the normalized data range\n",
    "print(f\"X_train normalized range: [{X_train_scaled.min().item():.4f}, {X_train_scaled.max().item():.4f}]\")\n",
    "print(f\"y_train normalized range: [{y_train_scaled.min().item():.4f}, {y_train_scaled.max().item():.4f}]\")\n",
    "\n",
    "# ✅ Check your answer\n",
    "answer = {\n",
    "    'X_train_tensor': X_train_tensor,\n",
    "    'y_train_tensor': y_train_tensor,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'y_train_scaled': y_train_scaled,\n",
    "    'scale_range_min': X_train_scaled.min().item(),\n",
    "    'scale_range_max': X_train_scaled.max().item(),\n",
    "}\n",
    "checker.check_exercise(1, answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Activation Function\n",
    "***\n",
    "The next step is to choose an activation function for the model. The activation function introduces non-linearity to the model, allowing it to learn complex relationships in the data. The following table lists some common activation functions used in neural networks, along with their characteristics and best use cases:\n",
    "\n",
    "| Function | Formula | Range | PyTorch Implementation | Best Used For |\n",
    "|----------|---------|-------|-------------------|---------------|\n",
    "| ReLU | $\\displaystyle f(x) = \\max(0, x)$ | $\\displaystyle [0, \\infty)$ | `torch.nn.ReLU()` | Hidden layers in most networks |\n",
    "| Sigmoid | $\\displaystyle f(x) = \\frac{1}{1+e^{-x}}$ | $\\displaystyle (0, 1)$ | `torch.nn.Sigmoid()` | Binary classification, gates in LSTMs |\n",
    "| Tanh | $\\displaystyle f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $\\displaystyle (-1, 1)$ | `torch.nn.Tanh()` | Hidden layers when output normalization is needed |\n",
    "| Leaky ReLU | $\\displaystyle f(x) = \\max(\\alpha x, x)$ | $\\displaystyle (-\\infty, \\infty)$ | `torch.nn.LeakyReLU(negative_slope=0.01)` | Preventing \"dead neurons\" problem |\n",
    "| Softmax | $\\displaystyle f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$ | $\\displaystyle (0, 1)$ | `torch.nn.Softmax(dim=1)` | Multi-class classification output layer |\n",
    "\n",
    "The choice of activation function depends on the specific problem and the architecture of the neural network. \n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tips**:\n",
    "> - ReLU is the most commonly used activation function in hidden layers of deep networks due to its simplicity and effectiveness.\n",
    "> - The activation function for the output layer depends on the type of problem being solved (e.g., regression, binary classification, multi-class classification).\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\"/> **Common Mistakes to Avoid**: \n",
    "> - Mixing activation functions in the same layer (e.g., using ReLU and sigmoid together) can lead to unexpected behavior.\n",
    "> - Using activation functions that saturate (like sigmoid) in hidden layers can lead to vanishing gradients, making training difficult.\n",
    "> - Forgetting to apply the activation function to the output layer can lead to incorrect predictions (e.g., not using softmax for multi-class classification).\n",
    "> - Not considering the range of the output when choosing the activation function (e.g., using sigmoid for regression tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧠 Quiz 1: Choosing the right activation function\")\n",
    "quizzer.run_quiz(1)\n",
    "\n",
    "print(\"\\n🧠 Quiz 2: Combining activation functions\")\n",
    "quizzer.run_quiz(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Model\n",
    "***\n",
    "The next step is to define the model architecture. In order to create a Neural Network we need to stack multiple neurons together. This is known as a **layer**. A layer is a collection of neurons that work together to process the input data. A simple ANN is formed by three types of layers:\n",
    "   - **Input Layer**: Receives the input data.\n",
    "   - **Hidden Layers**: Intermediate layers that process the data.\n",
    "   - **Output Layer**: Produces the final output.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/layers.png\" width=\"35%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The following table summarises the different types of layers available in PyTorch:\n",
    "\n",
    "| Layer Type | Class | Description | Common Uses |\n",
    "|------------|-------|-------------|------------|\n",
    "| Fully Connected | `torch.nn.Linear(in_features, out_features)` | Standard dense layer | Classification, regression |\n",
    "| Convolutional | `torch.nn.Conv2d(in_channels, out_channels, kernel_size)` | Spatial feature extraction | Image processing |\n",
    "| Recurrent | `torch.nn.RNN(input_size, hidden_size)` | Sequential data processing | Time series, text |\n",
    "| LSTM | `torch.nn.LSTM(input_size, hidden_size)` | Long-term dependencies | Complex sequences |\n",
    "| Embedding | `torch.nn.Embedding(num_embeddings, embedding_dim)` | Word vector representations | NLP tasks |\n",
    "| BatchNorm | `torch.nn.BatchNorm2d(num_features)` | Normalizes layer inputs | Training stability |\n",
    "| Dropout | `torch.nn.Dropout(p=0.5)` | Randomly zeros elements | Regularization |\n",
    "\n",
    "The choice of layer type depends on the specific problem and the architecture of the neural network. For example, convolutional layers are commonly used in image processing tasks, while recurrent layers are used for sequential data processing.\n",
    "\n",
    "### Number of Layers and Neurons\n",
    "***\n",
    "The number of layers and neurons in each layer is a hyperparameter that needs to be tuned. The following table summarises the common practices for choosing the number of layers and neurons:\n",
    "\n",
    "| Layer Type | Common Practices |\n",
    "|----------------|------------------|\n",
    "| Input Layer | Number of neurons = number of input features |\n",
    "| Hidden Layers | 1-3 hidden layers are common for most tasks. More complex tasks may require more layers. |\n",
    "| Output Layer | Number of neurons = number of output features (e.g., 1 for regression, number of classes for classification) |\n",
    "| Number of Neurons | Common practices: 2^n, where n is the number of layers. A common practice is to start with a number of neurons equal to the number of input features and then reduce the number of neurons in each subsequent layer. |\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\" /> **Tips**:\n",
    "> - Start with a simple architecture and gradually increase complexity as needed.\n",
    "> - The number of neurons in each layer can be adjusted based on the complexity of the problem.\n",
    "> - Use activation functions after each layer to introduce non-linearity.\n",
    "> - Experiment with different layer types and configurations to find the best architecture for your problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz 3: Network Width\n",
    "print(\"\\n🧠 Quiz 3: Understanding Network Width for Inverse Kinematics\")\n",
    "quizzer.run_quiz(3)\n",
    "\n",
    "# Quiz 4: Network Depth\n",
    "print(\"\\n🧠 Quiz 4: Understanding Network Depth for Inverse Kinematics\")\n",
    "quizzer.run_quiz(4)\n",
    "\n",
    "# Quiz 5: Regularization Techniques\n",
    "print(\"\\n🧠 Quiz 5: Regularization Techniques for Kinematics Models\")\n",
    "quizzer.run_quiz(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising Weights and Biases\n",
    "***\n",
    "\n",
    "In the previous session we looked at the concept of weights and biases. With our Perceptron we initialised the weights and biases to random values. In PyTorch, we can use different methods to initialise the weights and biases of a neural network.\n",
    "\n",
    "The importance of initialising weights and biases lies in the fact that they can significantly affect the convergence speed and performance of the neural network. Proper initialisation can help prevent issues such as vanishing or exploding gradients, which can hinder the training process.\n",
    "\n",
    "| Initialisation Method | Formula | PyTorch Code | Description |\n",
    "|-----------------------|----------|--------------|-------------|\n",
    "| Xavier/Glorot Initialisation | $\\displaystyle W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$ | `torch.nn.init.xavier_uniform_(tensor)` | Suitable for sigmoid and tanh activations. |\n",
    "| He Initialisation | $\\displaystyle W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Normal Initialisation | $\\displaystyle W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}})$ | `torch.nn.init.kaiming_normal_(tensor)` | Suitable for ReLU activations. |\n",
    "| Kaiming Uniform Initialisation | $\\displaystyle W \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$ | `torch.nn.init.kaiming_uniform_(tensor)` | Suitable for ReLU activations. |\n",
    "| Zero Initialisation | $\\displaystyle W = 0$ | `torch.nn.init.zeros_(tensor)` | All weights are set to zero. Not recommended. |\n",
    "| Random Initialisation | $\\displaystyle W \\sim \\mathcal{U}(-1, 1)$ | `torch.nn.init.uniform_(tensor)` | Weights are randomly initialised between -1 and 1. |\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tips**:\n",
    "> - Use Xavier or He initialisation for most cases, as they are designed to maintain the variance of activations across layers.\n",
    "> - Avoid zero initialisation, as it can lead to symmetry problems where all neurons learn the same features.\n",
    "> - PyTorch uses Kaiming initialisation by default for `torch.nn.Linear` layers, which is suitable for ReLU activations.\n",
    "> - Experiment with different initialisation methods to see their impact on training speed and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Model Creation with Weight Initialization 🎯\n",
    "# In this exercise, you will:\n",
    "# 1. Create a simple neural network model using PyTorch\n",
    "# 2. Initialize weights and biases properly\n",
    "# 3. Define layers with appropriate activation functions\n",
    "# 4. Implement a forward method\n",
    "\n",
    "class RobotArmNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialize a neural network for robotic arm inverse kinematics\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in the hidden layer\n",
    "            output_size: Number of output features\n",
    "        \"\"\"\n",
    "        # Initialize the parent class\n",
    "        # Your code here\n",
    "        \n",
    "        # Define the layers of your neural network (simple architecture to avoid overfitting)\n",
    "        self.fc1 = # Your code here\n",
    "        self.hidden_activation = # Your code here\n",
    "        self.fc2 = # Your code here\n",
    "        \n",
    "        # Initialize the weights using appropriate initialization techniques\n",
    "        # He/Kaiming initialization for layers with ReLU activation\n",
    "        # Your code here\n",
    "        # Your code here\n",
    "        \n",
    "        # Xavier/Glorot initialization for the output layer\n",
    "        # Your code here\n",
    "        # Your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Process input through first fully connected layer and activation function\n",
    "        x = # Your code here\n",
    "        \n",
    "        # Process through output layer (no activation - we want raw values for regression)\n",
    "        x = # Your code here\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize your model with appropriate dimensions\n",
    "model = # Your code here\n",
    "\n",
    "# Print your model architecture\n",
    "print(model)\n",
    "\n",
    "# Print weight statistics to verify initialization\n",
    "print(\"\\nWeight initialization validation:\")\n",
    "print(f\"First layer weight stats: mean={model.fc1.weight.mean().item():.4f}, std={model.fc1.weight.std().item():.4f}\")\n",
    "print(f\"First layer bias: mean={model.fc1.bias.mean().item():.4f}, std={model.fc1.bias.std().item():.4f}\")\n",
    "print(f\"Output layer weight stats: mean={model.fc2.weight.mean().item():.4f}, std={model.fc2.weight.std().item():.4f}\")\n",
    "print(f\"Output layer bias: mean={model.fc2.bias.mean().item():.4f}, std={model.fc2.bias.std().item():.4f}\")\n",
    "\n",
    "# ✅ Check your answer\n",
    "answer = {\n",
    "    'model': model,\n",
    "    'input_layer_size': model.fc1.in_features,\n",
    "    'output_layer_size': model.fc2.out_features,\n",
    "    'activation_type': model.hidden_activation.__class__,\n",
    "    'fc1_weight_stats': {\n",
    "        'mean': model.fc1.weight.mean().item(),\n",
    "        'std': model.fc1.weight.std().item()\n",
    "    },\n",
    "    'fc2_weight_stats': {\n",
    "        'mean': model.fc2.weight.mean().item(),\n",
    "        'std': model.fc2.weight.std().item()\n",
    "    },\n",
    "    'fc1_bias_stats': {\n",
    "        'mean': model.fc1.bias.mean().item(),\n",
    "        'std': model.fc1.bias.std().item()\n",
    "    },\n",
    "    'fc2_bias_stats': {\n",
    "        'mean': model.fc2.bias.mean().item(),\n",
    "        'std': model.fc2.bias.std().item()\n",
    "    }\n",
    "}\n",
    "checker.check_exercise(2, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Choose Optimiser\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Optimisers are algorithms used to update the model parameters during training to minimise the loss function.\n",
    "\n",
    "The next step is to choose an optimiser for the model.\n",
    "\n",
    "The optimiser algorithm is used to update the model parameters during training. Most optimizers use a version of gradient descent to update the model parameters. The goal of the optimiser is to minimize the loss function by adjusting the weights and biases of the model. The most commonly used optimizers include:\n",
    "\n",
    "| Optimizer | PyTorch Implementation | Best Used For |\n",
    "|-----------|---------------------|--------------|\n",
    "| Stochastic Gradient Descent (SGD) | `torch.optim.SGD(params, lr)` | Simple problems, good with momentum |\n",
    "| Adam | `torch.optim.Adam(params, lr)` | Most deep learning tasks |\n",
    "| RMSProp | `torch.optim.RMSprop(params, lr)` | Deep neural networks |\n",
    "| Adagrad | `torch.optim.Adagrad(params, lr)` | Sparse data tasks |\n",
    "| AdamW | `torch.optim.AdamW(params, lr)` | When regularization is important |\n",
    "\n",
    "The Adam optimiser is a popular choice for training deep learning models due to its efficiency and effectiveness. It combines the benefits of both SGD and RMSProp, making it suitable for a wide range of tasks.\n",
    "\n",
    "### Learning Rate\n",
    "***\n",
    "The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function. A small learning rate may lead to slow convergence, while a large learning rate may cause the model to diverge. It is important to choose an appropriate learning rate for the optimizer to work effectively\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/learning_rate.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "## Step 7: Choose Loss Function\n",
    "***\n",
    "The next step is to choose a loss function for the model. The choice of loss function depends on the type of problem being solved. The loss function measures how well the model is performing and guides the optimisation process. The most commonly used loss functions include:\n",
    "\n",
    "| Loss Function | PyTorch Implementation | Best Used For |\n",
    "|---------------|---------------------|--------------|\n",
    "| Mean Squared Error (MSE) | `torch.nn.MSELoss()` | Regression tasks |\n",
    "| Mean Absolute Error (MAE) | `torch.nn.L1Loss()` | Regression tasks |\n",
    "| Binary Cross-Entropy | `torch.nn.BCELoss()` | Binary classification tasks |\n",
    "| Categorical Cross-Entropy | `torch.nn.CrossEntropyLoss()` | Multi-class classification tasks |\n",
    "| Hinge Loss | `torch.nn.HingeEmbeddingLoss()` | Support Vector Machines (SVM) |\n",
    "| Kullback-Leibler Divergence | `torch.nn.KLDivLoss()` | Probabilistic models |\n",
    "\n",
    "The loss works in conjunction with the optimiser. While there are loss functions that can work for the same task, the choice of loss will have an effect on the final performance of the model. For instance, using MSE (L2-Norm) loss for a regression task will penalise larger errors more than smaller ones, while MAE (L1-Norm) loss treats all errors equally. This can lead to different model performance depending on the distribution of the data.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/losses.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Tips**:\n",
    "> - Choose a loss function that is appropriate for the type of problem being solved (e.g., regression, classification).\n",
    "> - Monitor the loss during training to ensure that the model is converging and not overfitting.\n",
    "> - Experiment with different loss functions to see their impact on model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Optimizer and Loss Function Selection 🎯\n",
    "# In this exercise, you will:\n",
    "# 1. Select an appropriate optimizer for your model\n",
    "# 2. Choose a suitable loss function\n",
    "# 3. Set the learning rate\n",
    "\n",
    "# Create an Adam optimizer for your model\n",
    "optimizer = # Your code here\n",
    "\n",
    "# Create a Mean Squared Error loss function\n",
    "loss_function = # Your code here\n",
    "\n",
    "# Store the optimizer and loss function in the model for easy access\n",
    "model.optimizer = optimizer\n",
    "model.loss_function = loss_function\n",
    "\n",
    "# Print the optimizer and loss function configuration\n",
    "print(f\"Optimizer: {type(model.optimizer).__name__}\")\n",
    "print(f\"Learning rate: {model.optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Loss function: {type(model.loss_function).__name__}\")\n",
    "\n",
    "# ✅ Check your answer\n",
    "answer = {\n",
    "    'optimizer_type': type(optimizer),\n",
    "    'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "    'loss_function_type': type(loss_function)\n",
    "}\n",
    "checker.check_exercise(3, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 and 9: Create Training Loop and Fit Model\n",
    "***\n",
    "The training loop implements the key steps for training a neural network model:\n",
    "\n",
    "| Step | Description | Code Example |\n",
    "|------|-------------|--------------|\n",
    "| 1. Forward Pass | Pass input data through model to generate predictions | `predictions = model(inputs)` |  \n",
    "| 2. Loss Computation | Calculate loss between predictions and targets | `loss = criterion(predictions, targets)` |\n",
    "| 3. Backward Pass | Compute gradients through backpropagation | `loss.backward()` |\n",
    "| 4. Parameter Updates | Update model parameters using optimizer | `optimizer.step()` |\n",
    "| 5. Gradient Reset | Zero out gradients for next iteration | `optimizer.zero_grad()` |\n",
    "\n",
    "The next step is to fit the model using the training data. The model is trained for a specified number of epochs, and the training and validation loss is monitored during training. The number of epochs is a hyperparameter that determines how many times the model will be trained on the entire training dataset.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Training Loop Structure\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Forward Pass\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # 2. Loss Computation\n",
    "    loss = criterion(predictions, targets)\n",
    "    \n",
    "    # 3. Backward Pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Parameter Updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 5. Gradient Reset\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Creating a Training Loop 🎯\n",
    "# In this exercise, you will:\n",
    "# 1. Create a training loop for your neural network\n",
    "# 2. Implement forward and backward passes\n",
    "# 3. Monitor training and validation loss\n",
    "\n",
    "def train_model(model, \n",
    "                train_features, \n",
    "                train_targets, \n",
    "                val_features, \n",
    "                val_targets, \n",
    "                epochs=100):\n",
    "    \"\"\"\n",
    "    Train a neural network model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_features: Training features\n",
    "        train_targets: Training targets\n",
    "        val_features: Validation features\n",
    "        val_targets: Validation targets\n",
    "        epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Initialize lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Put model in training mode\n",
    "    # Your code here\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        # 1. Zero gradients\n",
    "        # Your code here\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        predictions = # Your code here\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = # Your code here\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        # Your code here\n",
    "        \n",
    "        # 5. Update weights\n",
    "        # Your code here\n",
    "        \n",
    "        # 6. Store training loss\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # 7. Compute validation loss\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        with torch.no_grad(): # No need to track gradients for validation\n",
    "            val_predictions = model(val_features)\n",
    "            val_loss = model.loss_function(val_predictions, val_targets).item()\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        # Set model back to training mode\n",
    "        model.train()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Run training for 100 epochs\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_features=X_train_scaled,\n",
    "    train_targets=y_train_scaled,\n",
    "    val_features=X_val_scaled,\n",
    "    val_targets=y_val_scaled,\n",
    "    epochs=300\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss')\n",
    "ax.plot(val_losses, label='Validation Loss')\n",
    "utils.plotting.make_fig_pretty(ax, title='Loss vs Epochs', xlabel='Epochs', ylabel='Loss',ctab=True)\n",
    "plt.show()\n",
    "\n",
    "# ✅ Check your answer\n",
    "answer = {\n",
    "    'train_losses': train_losses[-1],\n",
    "    'val_losses': val_losses[-1],\n",
    "    'loss_trend': train_losses[0] > train_losses[-1],\n",
    "    'overfit_check': val_losses[-1] <= val_losses[0] * 1.5  # Should not have increased much\n",
    "}\n",
    "checker.check_exercise(4, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, and Early Stopping\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Overfitting occurs when a model learns the training data too well, including noise and outliers, leading to poor generalisation on unseen data. Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "As we can see in the following figure, the training loss decreases over time, while the validation loss follows a similar trend. However, the validation loss starts to slowly deviate from the training loss after a certain number of epochs. This indicates that the model is starting to overfit the training data. The point at which the validation loss starts to increase is known as the \"early stopping\" point. This is the point at which we should stop training the model to prevent overfitting.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/over_under_fit.png\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model\n",
    "***\n",
    "The next step is to evaluate the model using the validation and test data. The model is evaluated on the validation set during training to monitor its performance and prevent overfitting. \n",
    "\n",
    "Since we are training a model with MSE loss, we can also plot the predicted output against the actual output to see how well the model is performing. The predicted output should be close to the actual output, and the points should be clustered around the diagonal line. If the points are scattered far from the diagonal line, it indicates that the model is not performing well.\n",
    "\n",
    "We can also compute the R-squared value to quantify the performance of the model. The R-squared value is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The R-squared value ranges from 0 to 1, where 0 indicates that the model does not explain any of the variance in the data, and 1 indicates that the model explains all of the variance in the data.\n",
    "\n",
    "For this step we are going to use the test set to evaluate the model. \n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 3**: Evaluate Model\n",
    "\n",
    "```python\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Model Evaluation 🎯\n",
    "# In this exercise, you will:\n",
    "# 1. Evaluate your trained model on the test set\n",
    "# 2. Calculate R-squared score to measure model performance\n",
    "# 3. Visualize actual vs. predicted values for one joint\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict on the test set without computing gradients\n",
    "with torch.no_grad():\n",
    "    test_predictions = # Your code here\n",
    "    \n",
    "    # Calculate the test loss\n",
    "    test_loss = # Your code here\n",
    "    \n",
    "    # Convert predictions and targets back to original scale\n",
    "    test_predictions_original = # Your code here\n",
    "    test_targets_original = # Your code here\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r2_score = utils.ml.r2_score(test_targets_original, test_predictions_original)  \n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "print(f\"R-squared Score: {r2_score:.4f}\")\n",
    "\n",
    "# Visualize actual vs. predicted values for the shoulder pitch joint (first joint)\n",
    "fig, axes = plt.subplots(figsize=(16, 20), nrows=5)\n",
    "\n",
    "for ix, joint in enumerate(y_test.columns):\n",
    "    axes[ix].plot(test_targets_original[:, ix], test_predictions_original[:, ix], 'o', fillstyle='none', markersize=2)\n",
    "    axes[ix].plot(test_targets_original[:, ix], test_targets_original[:, ix], 'r--')\n",
    "\n",
    "    utils.plotting.make_fig_pretty(axes[ix], title=f\"{joint}\", ylabel='Predicted',\n",
    "                                   xtick_fsize=10, ytick_fsize=10,\n",
    "                                   title_fsize=12, xlabel_fsize=10)\n",
    "\n",
    "    if ix == 4:\n",
    "        axes[ix].set_xlabel('ACTUAL')\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Check your answer\n",
    "answer = {\n",
    "    'test_loss': test_loss.item(),\n",
    "    'r2_score': r2_score,\n",
    "    'predictions_shape': test_predictions_original.shape,\n",
    "    'values_match': test_predictions_original.shape == test_targets_original.shape\n",
    "}\n",
    "checker.check_exercise(5, answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
