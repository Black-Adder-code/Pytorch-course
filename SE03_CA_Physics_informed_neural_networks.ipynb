{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/se_03b.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE03_CA_Physics_informed_neural_networks.ipynb)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> Look for hints if you get stuck\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/success.svg\" width=\"20\" /> Compare your solution with the provided answers\n",
    "- <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt -O colab_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=3 -i colab_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import inspect\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "    \n",
    "checker = utils.core.ExerciseChecker(\"SE03P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to PINNs\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Physics-Informed Neural Networks (PINNs) are neural networks that are trained to solve supervised learning tasks while respecting physical laws described by partial differential equations.\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) are a class of deep learning models that integrate physical laws, typically expressed as partial differential equations (PDEs), directly into the learning process. Instead of relying solely on data, PINNs leverage known physics to constrain the model, allowing for more robust learning, especially in scenarios where data is scarce or noisy.\n",
    "\n",
    "PINNs combine two major concepts:\n",
    "\n",
    "| Component | Description | Role |\n",
    "|-----------|-------------|------|\n",
    "| **Neural Networks** | Deep learning models that can approximate complex functions | Learn patterns from data |\n",
    "| **Physical Laws** | Mathematical equations describing system behavior | Enforce physical constraints |\n",
    "\n",
    "## 1.1 Why PINNs?\n",
    "***\n",
    "Traditional numerical methods for solving PDEs face several challenges:\n",
    "\n",
    "| Challenge | Traditional Methods | PINN Solution |\n",
    "|-----------|---------------------|---------------|\n",
    "| **Computational Cost** | High for complex geometries | Efficient once trained |\n",
    "| **Mesh Requirements** | Need fine meshes | Meshless approach |\n",
    "| **Limited Data** | Require complete boundary conditions | Can work with sparse data |\n",
    "| **High Dimensions** | Suffer from curse of dimensionality | Better scaling with dimensions |\n",
    "| **Generalisation** | Limited to specific problems | Generalises to new conditions |\n",
    "\n",
    "\n",
    "# 2. Case Study: Navier-Stokes Equations\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: The Navier-Stokes equations are partial differential equations that describe the motion of viscous fluid substances, forming the basis of fluid dynamics.\n",
    "\n",
    "These equations govern the motion of incompressible fluids in 2D:\n",
    "\n",
    "**Momentum equations:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial u}{\\partial t} + \\lambda_1 (u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y}) &= -\\frac{\\partial p}{\\partial x} + \\lambda_2 (\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}) \\quad \\text{(momentum in x-direction)} \\\\\n",
    "\\frac{\\partial v}{\\partial t} + \\lambda_1 (u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y}) &= -\\frac{\\partial p}{\\partial y} + \\lambda_2 (\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}) \\quad \\text{(momentum in y-direction)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Continuity equation (incompressibility condition):**\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0\n",
    "$$\n",
    "\n",
    "**Meaning of terms:**\n",
    "- $u(x, y, t)$: horizontal velocity\n",
    "- $v(x, y, t)$: vertical velocity\n",
    "- $p(x, y, t)$: pressure\n",
    "- $\\lambda_1$: convection coefficient (usually 1)\n",
    "- $\\lambda_2 = \\nu $: kinematic viscosity\n",
    "\n",
    "These equations state that:\n",
    "- Fluids accelerate due to pressure differences and internal friction (viscosity)\n",
    "- Mass is conserved â†’ the flow remains incompressible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Flow past a cylinder\n",
    "***\n",
    "In this notebook we are going to explore a realistic scenario of incompressible fluid flow as described by the ubiquitous Navier-Stokes equations. Navier-Stokes equations describe the physics of many phenomena of scientific and engineering. Often, the Navier-Stokes equations are solved using numerical methods, such as finite element or finite volume methods. However, these methods can be computationally expensive and time-consuming, especially for complex geometries and boundary conditions.\n",
    "In this workshop, we will use a dataset of incompressible fluid flow around a cylinder. The dataset is generated using a finite volume method and contains the velocity and pressure fields of the fluid flow. The dataset consists of the following variables:\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| u        | x-component of velocity |\n",
    "| v        | y-component of velocity |\n",
    "| p        | pressure |\n",
    "| t        | time |\n",
    "\n",
    "The dataset was prepared using the following simulation parameters:\n",
    "- **Domain**: \\( [-15, 25] \\times [-8, 8] \\)\n",
    "- **Reynolds number**: \\( Re = 100 \\)\n",
    "- **Numerical method**: Spectral/hp-element solver (NekTar)\n",
    "- **Mesh**: 412 triangular elements, 10th-order basis functions\n",
    "- **Integration**: Third-order stiff scheme until steady vortex shedding\n",
    "\n",
    "\n",
    "For this problem, we want to predict the Convective term $\\lambda_1$, the viscous term $\\lambda_2$, as well as a reconstruction of the pressure field $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('cylinder',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=False,\n",
    "                                   remove_compressed=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(dataset_path)\n",
    "u_star = data['U_star'] # velocity n x 2 x time\n",
    "p_star = data['p_star'] # pressure n x time\n",
    "x_star = data['X_star'] # coordinates n x 2\n",
    "time = data['t'] # time n x 1\n",
    "\n",
    "print(f'We have {u_star.shape[0]} points in space, {u_star.shape[1]} dimensions and {u_star.shape[2]} time steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plotting.wake_cylinder_interactive(x_star, u_star, p_star, time, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the dataset\n",
    "***\n",
    "As you can see in the image above, the simulation has been cropped to a smaller domain that does not include the cylinder. Thus, with this dataset we are only using 1% of the total data for training. This is to highlight the ability of PINNs to learn from limited data. In practice, you would typically use a larger portion of the dataset for training. Thus, we are not going to split the dataset into training, validation, and test sets. Instead, we will use the entire dataset for training and testing. This is a common practice in PINNs, where the model is trained on a small portion of the data and then tested on the entire dataset.\n",
    "\n",
    "The training data consists of the following parameters:\n",
    "- Region: Small rectangle downstream of cylinder\n",
    "- Sampled: \\( u(x, y, t), v(x, y, t) \\)\n",
    "- Size: 5,000 points (\\~1% of simulation)\n",
    "- **No pressure data used**\n",
    "\n",
    "To prepare the data we need to load the dataset and extract the relevant variables. Furthermore, we need to reshape the data to be compatible with the PINN model. \n",
    "\n",
    "## 3.1 Normalisation\n",
    "***\n",
    "For PINNs, the features cannot be normalised in the same way as in traditional machine learning. Standard normalization in PINNs is tricky because physical equations must remain consistent with the scaled variables. The normalisation should be applied to the loss function considering the physics of the problem. This is a more complex process and requires a deeper understanding of the physical equations involved. \n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/list.svg\" width=\"20\" /> In this workshop, we will not cover this topic in detail, but it is important to keep in mind that normalisation in PINNs is not as straightforward as in traditional machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Prepare the data for training ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Create a meshgrid-like structure for the x, y, and t coordinates\n",
    "# 2. Flatten the velocity and pressure data into NT x 1 arrays\n",
    "# 3. Randomly sample N points from the flattened data\n",
    "# 4. Convert the sampled data into PyTorch tensors\n",
    "\n",
    "N, T = x_star.shape[0], time.shape[0] # number of points in space and time\n",
    "\n",
    "# Create coordinate grids \n",
    "x_flat = x_star[:, 0]  # Extract x coordinates\n",
    "y_flat = x_star[:, 1]  # Extract y coordinates\n",
    "t_flat = time.flatten() # Flatten time array\n",
    "\n",
    "# Create meshgrid-like structures for visualization\n",
    "x_coords = # Your code here\n",
    "y_coords = # Your code here\n",
    "time_coords = # Your code here\n",
    "\n",
    "# Extract velocity and pressure data\n",
    "u_vals = # Your code here\n",
    "v_vals = # Your code here\n",
    "p_vals = # Your code here\n",
    "\n",
    "# Flatten into NT x 1 arrays\n",
    "x = # Your code here\n",
    "y = # Your code here\n",
    "t = # Your code here\n",
    "u = # Your code here\n",
    "v = # Your code here\n",
    "p = # Your code here\n",
    "\n",
    "idx = # Your code here\n",
    "x_train = x[idx, :]\n",
    "y_train = y[idx, :]\n",
    "t_train = t[idx, :]\n",
    "u_train = u[idx, :]\n",
    "v_train = v[idx, :]\n",
    "\n",
    "# Preparing the data as tensors\n",
    "x_train = # Your code here\n",
    "y_train = # Your code here\n",
    "t_train = # Your code here\n",
    "u_train = # Your code here\n",
    "v_train = # Your code here\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'x_train': x_train,\n",
    "    'y_train': y_train,\n",
    "    't_train': t_train,\n",
    "    'u_train': u_train,\n",
    "    'v_train': v_train,\n",
    "    'data_shape': x_train.shape[0]\n",
    "}\n",
    "checker.check_exercise(6, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PINN Architecture\n",
    "***\n",
    "In this workshop we are going to use a Physics-Informed Neural Network (PINN) to solve the Navier-Stokes equations. The PINN model is a neural network that is trained to satisfy the Navier-Stokes equations, as well as the boundary conditions of the problem. The PINN model consists of the following components:\n",
    "\n",
    "| Component | Purpose | Implementation |\n",
    "|-----------|---------|----------------|\n",
    "| **Input Layer** | Takes spatial/temporal coordinates | $(x, y, t)$ coordinates |\n",
    "| **Hidden Layers** | Learn the underlying patterns | Multiple fully connected layers |\n",
    "| **Output Layer** | Predicts physical quantities | Velocity and pressure fields |\n",
    "| **Physics Loss** | Enforces PDE constraints | Automatic differentiation |\n",
    "\n",
    "***\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/pinn.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "For the connected layers we are going to use the following activation functions:\n",
    "\n",
    "| Layer | Activation Function |\n",
    "|-------|---------------------|\n",
    "| Input Layer | Tanh |\n",
    "| Hidden Layers | Tanh |\n",
    "| Output Layer | Tanh |\n",
    "\n",
    "The choice of activation function for the output layer is important, as it can affect the range of the output values. In this case, we are using Tanh to ensure that the output values are in the range [-1, 1]. This is important for the PINN model, as we want to ensure that the predicted values are in the same range as the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Model Creation with Weight Initialization ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a class for the Navier-Stokes PINN model\n",
    "# 2. Initialize the model with a specified number of hidden layers and neurons\n",
    "# 3. Use Xavier initialization for the weights and biases of each layer\n",
    "# 4. Define the forward pass to compute the velocity and pressure outputs\n",
    "# 5. Split the output into u, v, and p components \n",
    "\n",
    "class NavierStokesPINN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=20, num_layers=9):\n",
    "        super().__init__()\n",
    "        self.nu = 0.01  # kinematic viscosity\n",
    "        \n",
    "        # Neural network architecture\n",
    "        layers = []\n",
    "        # Input layer: x, y, t\n",
    "        input_layer = # Your code here\n",
    "        \n",
    "        # Your code here :Initialize weights and biases\n",
    "\n",
    "\n",
    "        layers.append(input_layer)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            hidden_layer = # Your code here\n",
    "           \n",
    "            # Your code here :Initialize weights and biases\n",
    "\n",
    "            layers.append(hidden_layer)\n",
    "            \n",
    "        # Output layer:  u,v,p because we are enforcing the continuity equation\n",
    "        output_layer = # Your code here\n",
    "        \n",
    "        # Your code here :Initialize weights and biases\n",
    "\n",
    "        layers.append(output_layer)\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        # Combine inputs\n",
    "        xyz = # Your code here\n",
    "        \n",
    "        # Forward pass through network\n",
    "        for i in range(len(self.net)-1):\n",
    "            xyz = # Your code here\n",
    "        \n",
    "        # Final layer without activation\n",
    "        output = # Your code here\n",
    "        \n",
    "        # Split output into u, v, p\n",
    "        u = # Your code here\n",
    "        v = # Your code here\n",
    "        p = # Your code here\n",
    "        \n",
    "        return u, v, p\n",
    "\n",
    "model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'model': model,\n",
    "    'hidden_size': model.net[0].out_features,\n",
    "    'input_size': model.net[0].in_features,\n",
    "    'output_size': model.net[-1].out_features,\n",
    "    'num_layers': len(model.net)\n",
    "}\n",
    "checker.check_exercise(7, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. PINN Loss Function\n",
    "***\n",
    "When working with PINNs, the loss function is a crucial component that combines data loss and physics loss. The data loss measures how well the model fits the training data, while the physics loss measures how well the model satisfies the physical constraints defined by the PDEs. Thus, the loss function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{Data Loss} + \\text{Physics Loss}$$\n",
    "\n",
    "$$\\text{Data Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (u_i - u_{\\text{pred}, i})^2 + (v_i - v_{\\text{pred}, i})^2$$\n",
    "$$\\text{Physics Loss} = \\frac{1}{M} \\sum_{j=1}^{M} (f_{u,j}^2 + f_{v,j}^2 + f_{c,j}^2)$$\n",
    "\n",
    "Where:\n",
    "- $f_u = \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)$\n",
    "- $f_v = \\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left( \\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} \\right)$\n",
    "- $f_c = \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y}$\n",
    "- $f_c$: Continuity equation (incompressibility condition)\n",
    "- $f_u$: Momentum equation in x-direction\n",
    "- $f_v$: Momentum equation in y-direction\n",
    "- $N$: Number of data points in the training set\n",
    "- $M$: Number of points in the physics loss (collocation points)\n",
    "\n",
    "The physics loss ensures that the solution satisfies the Navier-Stokes equations, while the data loss ensures the solution matches known data points. This dual optimization approach is what makes PINNs powerful for solving PDEs.\n",
    "\n",
    "## 5.1 Automatic Differentiation in PINNs\n",
    "***\n",
    "One of the key features of PINNs is their ability to automatically compute derivatives using automatic differentiation (autograd). This is crucial for enforcing physical constraints defined by PDEs.\n",
    "\n",
    "### 5.1.1 How Automatic Differentiation Works\n",
    "When we use `torch.autograd.grad()`, PyTorch computes derivatives through the computational graph:\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: Basic usage of `torch.autograd.grad()`\n",
    "\n",
    "```python\n",
    "u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "```\n",
    "\n",
    "The `torch.autograd.grad()` function returns a tuple of gradients. The first element is the gradient of `u` with respect to `t`. \n",
    "\n",
    "| Parameter | Purpose | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `u` | Target tensor | The output we want to differentiate |\n",
    "| `t` | Source tensor | The variable we're differentiating with respect to |\n",
    "| `grad_outputs` | Scaling factor | Usually ones, for direct gradient computation |\n",
    "| `create_graph` | Enable higher derivatives | Needed for second derivatives |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Compute Residuals ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a function to compute the residuals of the Navier-Stokes equations\n",
    "# 2. Enable gradients for the input variables (x, y, t)\n",
    "# 3. Compute the first and second derivatives of the velocity and pressure fields\n",
    "# 4. Calculate the residuals for the u and v momentum equations and the continuity equation\n",
    "# 5. Return the residuals as outputs\n",
    "\n",
    "def compute_ns_residuals(model, x, y, t):\n",
    "    # Your code here: Enable gradients\n",
    "    \n",
    "\n",
    "    # Get predictions\n",
    "    u, v, p = model(x, y, t)\n",
    "\n",
    "    # First derivatives\n",
    "    u_t = # Your code here\n",
    "    u_x = # Your code here\n",
    "    u_y = # Your code here\n",
    "    \n",
    "    v_t = # Your code here\n",
    "    v_x = # Your code here\n",
    "    v_y = # Your code here\n",
    "    \n",
    "    p_x = # Your code here\n",
    "    p_y = # Your code here\n",
    "\n",
    "    # Second derivatives\n",
    "    u_xx = # Your code here\n",
    "    u_yy = # Your code here\n",
    "    \n",
    "    v_xx = # Your code here\n",
    "    v_yy = # Your code here\n",
    "\n",
    "    # Compute residuals\n",
    "    f_u = # Your code here\n",
    "    f_v = # Your code here\n",
    "    f_c = # Your code here\n",
    "\n",
    "    return f_u, f_v, f_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test case to verify the residual calculation\n",
    "test_x = torch.ones((5, 1), requires_grad=True)\n",
    "test_y = torch.ones((5, 1), requires_grad=True)\n",
    "test_t = torch.ones((5, 1), requires_grad=True)\n",
    "\n",
    "test_fu, test_fv, test_fc = compute_ns_residuals(model, test_x, test_y, test_t)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'f_u_shape': test_fu.shape,\n",
    "    'f_v_shape': test_fv.shape,\n",
    "    'f_c_shape': test_fc.shape,\n",
    "    'has_gradients': test_fu.requires_grad\n",
    "}\n",
    "checker.check_exercise(8, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PINN Optimiser and training\n",
    "***\n",
    "When training a PINN model, the choice of optimiser is crucial for achieving good performance. A common practice is to use a two-step approach: \n",
    "\n",
    "1. Use a standard optimiser (like Adam) to train the model, often called the \"warm-up\" phase. \n",
    "2. Switch to a more advanced optimiser (like L-BFGS) for fine-tuning.\n",
    "\n",
    "This approach allows for faster convergence in the initial phase and better performance in the final phase. \n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: The use of only Adam or L-BFGS is also possible, however, without the fine-tuning step the model will have an issue finding the right scale of the loss function. Resulting in a model that creates good qualitative results, but poor quantitative results.\n",
    "\n",
    "The warm-up phase follows the standard training process, where the model is trained using a standard optimiser (like Adam) for a certain number of epochs. The fine-tuning phase uses a more advanced optimiser (like L-BFGS) to refine the model parameters and improve performance.\n",
    "\n",
    "To use the L-BFGS optimiser, we need to define a closure function that computes the loss and gradients. The closure function is called by the optimiser to compute the loss and gradients, and it should return the loss value. The closure function should also zero out the gradients before computing the loss, as shown in the code snippet below.\n",
    "\n",
    "***\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Using L-BFGS Optimiser\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "model = PINNModel()\n",
    "# Define the loss function\n",
    "loss_function = PINNLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.LBFGS(model.parameters(), \n",
    "                                    lr=0.1,\n",
    "                                    max_iter=500,\n",
    "                                    max_eval=500,\n",
    "                                    tolerance_grad=1e-8,\n",
    "                                    tolerance_change=1e-8,\n",
    "                                    history_size=50,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "# Define the closure function\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(model)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(closure)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: The training loop ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Define a training function for the PINN model\n",
    "# 2. Use Adam optimizer for the first phase of training\n",
    "# 3. Use L-BFGS optimizer for the second phase of training\n",
    "# 4. Compute the loss as a combination of data loss and physics loss\n",
    "# 5. Use a closure function to compute the loss and gradients for L-BFGS\n",
    "\n",
    "def train_pinn(model, data, epochs=10000, use_lbfgs=True):\n",
    "    count = 0\n",
    "    bLBFGS = False\n",
    "    x, y, t, u_true, v_true = data\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = # Your code here: move model to device\n",
    "    \n",
    "    # Move data to device\n",
    "    x = # Your code here\n",
    "    y = # Your code here\n",
    "    t = # Your code here\n",
    "    u_true = # Your code here\n",
    "    v_true = # Your code here\n",
    "\n",
    "    # Adam optimization first\n",
    "    optimizer = # Your code here\n",
    "    mse_loss = # Your code here\n",
    "\n",
    "    losses = {\n",
    "        'data_loss': [],\n",
    "        'physics_loss': [],\n",
    "        'total_loss': [],\n",
    "        'lbfgs_data_loss': [],\n",
    "        'lbfgs_physics_loss': [],\n",
    "        'lbfgs_total_loss': []\n",
    "    }\n",
    "\n",
    "    def closure():\n",
    "        nonlocal count\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Data loss\n",
    "        u_pred, v_pred, _ = # Your code here\n",
    "        data_loss = # Your code here\n",
    "        \n",
    "        # Physics loss\n",
    "        f_u, f_v, f_c = # Your code here\n",
    "        physics_loss = # Your code here\n",
    "        \n",
    "        # Total loss\n",
    "        loss = # Your code here\n",
    "        \n",
    "        # Backpropagation\n",
    "        # Your code here: compute gradients\n",
    "\n",
    "        # Store losses\n",
    "        if bLBFGS:\n",
    "            losses['lbfgs_data_loss'].append(data_loss.item())\n",
    "            losses['lbfgs_physics_loss'].append(physics_loss.item())\n",
    "            losses['lbfgs_total_loss'].append(loss.item())\n",
    "\n",
    "            print('\\r' + f\"Training with LBFGS at epoch {count}: data Loss: {data_loss.item()}, \"\n",
    "                  f\"physics Loss: {physics_loss.item()}, \"\n",
    "                  f\"total Loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "        else:\n",
    "            losses['data_loss'].append(data_loss.item())\n",
    "            losses['physics_loss'].append(physics_loss.item())\n",
    "            losses['total_loss'].append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Train with Adam\n",
    "    pbar = tqdm(range(epochs), desc=\"Training with Adam\")\n",
    "    for _ in pbar:\n",
    "        # Your code here: compute loss \n",
    "        loss = \n",
    "        # Your code here: compute gradients\n",
    "        \n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'data_loss': losses['data_loss'][-1],\n",
    "            'physics_loss': losses['physics_loss'][-1],\n",
    "            'total_loss': losses['total_loss'][-1]\n",
    "        })\n",
    "    \n",
    "\n",
    "    # L-BFGS optimization\n",
    "    if use_lbfgs:\n",
    "        bLBFGS = True\n",
    "        optimizer = # Your code here: configure L-BFGS optimizer\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "# Test with a smaller number of epochs\n",
    "test_model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "data_tuple = (x_train[:10], y_train[:10], t_train[:10], u_train[:10], v_train[:10])\n",
    "\n",
    "test_trained_model, losses = train_pinn(test_model,\n",
    "                                        data_tuple, \n",
    "                                        epochs=2,\n",
    "                                        use_lbfgs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Check your answer\n",
    "# Analyze the training function to extract important components\n",
    "train_fn_code = inspect.getsource(train_pinn)\n",
    "\n",
    "# Check for key components in the code\n",
    "has_adam = 'Adam' in train_fn_code\n",
    "has_lbfgs = 'LBFGS' in train_fn_code\n",
    "has_closure = 'def closure' in train_fn_code\n",
    "has_data_loss = 'data_loss' in train_fn_code\n",
    "has_physics_loss = 'physics_loss' in train_fn_code\n",
    "computes_residuals = 'compute_ns_residuals' in train_fn_code\n",
    "updates_weights = 'optimizer.step()' in train_fn_code\n",
    "backprop = 'backward()' in train_fn_code\n",
    "\n",
    "answer = {\n",
    "    'function_code': train_fn_code,  # For deeper inspection\n",
    "    'has_adam': has_adam,\n",
    "    'has_lbfgs': has_lbfgs,\n",
    "    'uses_closure': has_closure,\n",
    "    'has_data_loss': has_data_loss,\n",
    "    'has_physics_loss': has_physics_loss,\n",
    "    'computes_residuals': computes_residuals,\n",
    "    'updates_weights': updates_weights,\n",
    "    'uses_backpropagation': backprop,\n",
    "    'learning_rate': 0.001 if 'lr=0.001' in train_fn_code else None,\n",
    "    'optimizer_params': {\n",
    "        'has_max_iter': 'max_iter=' in train_fn_code,\n",
    "        'has_line_search': 'line_search_fn=' in train_fn_code\n",
    "    }\n",
    "}\n",
    "checker.check_exercise(9, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop with the full dataset\n",
    "# Initialize model\n",
    "model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "\n",
    "# Train model\n",
    "trained_model, losses = train_pinn(\n",
    "    model,\n",
    "    (x_train, y_train, t_train, u_train, v_train),\n",
    "    epochs=10000,\n",
    "    use_lbfgs=True\n",
    ")\n",
    "\n",
    "# save the model\n",
    "torch.save(trained_model.state_dict(), 'se03_pinn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss history\n",
    "def plot_loss_history(losses):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(losses['data_loss'], label='Data Loss',)\n",
    "    ax.plot(losses['physics_loss'], label='Physics Loss')\n",
    "    ax.plot(losses['total_loss'], label='Total Loss')\n",
    "    ax.plot(losses['lbfgs_data_loss'], label='LBFGS Data Loss')\n",
    "    ax.plot(losses['lbfgs_physics_loss'], label='LBFGS Physics Loss')\n",
    "    ax.plot(losses['lbfgs_total_loss'], label='LBFGS Total Loss')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    utils.plotting.make_fig_pretty(ax, title='Loss History', xlabel='Epochs', ylabel='Loss')\n",
    "\n",
    "\n",
    "plot_loss_history(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluation\n",
    "\n",
    "Unlike standard models where performance is often quantified with scalar metrics like Mean Squared Error (MSE) or R-squared, PINNs are typically evaluated by comparing predicted and true physical fields over time (e.g., velocity and pressure fields in fluid dynamics).\n",
    "\n",
    "Here, we generate predictions from the trained model across all time steps and compare them to the ground truth values. Since we're modeling time-dependent flow behavior, we loop over each time snapshot and feed spatial coordinates (x, y) along with the corresponding time value t into the model. We compute and collect the predicted fields:\n",
    "\n",
    "- `u_pred`: Predicted horizontal velocity\n",
    "\n",
    "- `v_pred`: Predicted vertical velocity\n",
    "\n",
    "- `p_pred`: Predicted pressure\n",
    "\n",
    "These are then compared against the ground truth data from the test set for visual inspection and potential error metrics like relative L2 norm or RMSE (if applicable).\n",
    "\n",
    "> <img src=\"https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: When passing the data to the model, we need to ensure the tensors have `requires_grad=True`. This is important for the autograd engine to track operations on these tensors and compute gradients correctly. This is especially crucial when using optimisers like L-BFGS, which rely on gradient information to update model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10: Inference and Visualization ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Load the trained model\n",
    "# 2. Set the model to evaluation mode\n",
    "# 3. Move the model to CPU for inference\n",
    "# 4. Create input tensors with requires_grad=True\n",
    "# 5. Get predictions for u, v, and p\n",
    "# 6. Store predictions in lists and convert to arrays\n",
    "# 7. Reshape ground truth data\n",
    "\n",
    "model = NavierStokesPINN(hidden_size=20, num_layers=9)\n",
    "model.load_state_dict(torch.load('se03_pinn_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    u_pred = []\n",
    "    v_pred = []\n",
    "    p_pred = []\n",
    "    \n",
    "    for t_idx in range(time.shape[0]):\n",
    "        # Create input tensors with requires_grad=True\n",
    "        x_tensor = # Your code here\n",
    "        y_tensor = # Your code here\n",
    "        t_tensor = # Your code here\n",
    "        \n",
    "        # Get predictions - temporarily enable gradients\n",
    "        with torch.enable_grad():\n",
    "            u_t, v_t, p_t = # Your code here\n",
    "        \n",
    "        # Store predictions\n",
    "        u_pred.append(u_t.detach().cpu().numpy())\n",
    "        v_pred.append(v_t.detach().cpu().numpy())\n",
    "        p_pred.append(p_t.detach().cpu().numpy())\n",
    "    \n",
    "    # Convert to arrays and reshape\n",
    "    u_pred = # Your code here\n",
    "    v_pred = # Your code here\n",
    "    p_pred = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape ground truth data\n",
    "u_true = u_star[:, 0, :]  # Shape: (N, T)\n",
    "v_true = u_star[:, 1, :]  # Shape: (N, T)\n",
    "p_true = p_star  # Shape: (N, T)\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'u_pred_shape': u_pred.shape,\n",
    "    'v_pred_shape': v_pred.shape,\n",
    "    'p_pred_shape': p_pred.shape,\n",
    "    'requires_grad_used': True\n",
    "}\n",
    "checker.check_exercise(10, answer)\n",
    "\n",
    "# Create visualization\n",
    "utils.plotting.visualize_flow_comparison_interactive(\n",
    "    x_star, u_true, v_true, p_true,\n",
    "    u_pred, v_pred, p_pred, time,\n",
    "    figsize=(18, 4), \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
