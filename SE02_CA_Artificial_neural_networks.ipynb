{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/se_02.png)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/uom_fse_dl_workshop/blob/main/SE02_CA_Artificial_neural_networks.ipynb)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> Look for hints if you get stuck\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/success.svg\" width=\"20\" /> Compare your solution with the provided answers\n",
    "- <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/list.svg\" width=\"20\" /> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utils from GitHub\n",
    "!wget -q --show-progress https://raw.githubusercontent.com/CLDiego/uom_fse_dl_workshop/main/colab_utils.txt -O colab_utils.txt\n",
    "!wget -q --show-progress -x -nH --cut-dirs=3 -i colab_utils.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_path = Path.cwd()\n",
    "if str(repo_path) not in sys.path:\n",
    "    sys.path.append(str(repo_path))\n",
    "\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available. Please ensure you've enabled GPU in Runtime > Change runtime type\")\n",
    "\n",
    "\n",
    "challenger_temp = 0.56\n",
    "checker = utils.core.ExerciseChecker(\"SE02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artificial Neural Networks\n",
    "***\n",
    "Artificial Neural Networks (ANNs) are machine learning models inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers. They form the foundation of deep learning and typically contain:\n",
    "\n",
    "- **Input layer**: Receives the initial data/features\n",
    "- **Hidden layer(s)**: Processes the information\n",
    "- **Output layer**: Produces the final prediction/result\n",
    "\n",
    "Each neuron connects to neurons in the next layer through weighted connections that are adjusted during training to minimize the difference between predicted and actual outputs.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/ann.png\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "## 1.1 Neurons\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Neurons are the basic building blocks of ANNs, similar to biological neurons. Each neuron receives inputs, applies a transformation, and produces an output.\n",
    "\n",
    "The basic structure of a neuron can be seen below:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/non_linenar_neuron.png\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "### Neuron Output Formula\n",
    "\n",
    "The output of a neuron after applying the activation function is:\n",
    "$$ \\hat{y} = f(W \\cdot X + b)$$\n",
    "\n",
    "where: \n",
    "- $ \\hat{y} $  Predicted output of the neuron (may differ from actual output) \n",
    "- $ f $  Activation function (introduces non-linearity, e.g., sigmoid, tanh, ReLU) \n",
    "- $ W $  Weight vector (determines connection strengths) \n",
    "- $ X $  Input vector (features of the data) \n",
    "- $ b $  Bias term (shifts the activation function) \n",
    "- $ \\cdot $  Dot product between weight and input vectors \n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: Weights and biases are learnable parameters of a neural network. Weights determine the strength of connections between neurons, while biases allow the neuron to shift the activation function.\n",
    "\n",
    "### 1.1.1 Features and Targets\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: In machine learning, we refer to inputs as features and outputs as targets.\n",
    "\n",
    "In neural networks:\n",
    "\n",
    "- **Features (X)**: Input variables/attributes used for prediction\n",
    "- **Targets (y)**: Values we're trying to predict (ground truth labels)\n",
    "\n",
    "Now let's implement a neuron in Python using PyTorch. We'll create a `Neuron` class that handles the *forward pass* - passing inputs to the neuron and computing the output.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: We'll use PyTorch's `torch.nn.Module` as a base class and `torch.nn.Parameter` to define weights and biases, making them learnable parameters that can be updated during training.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 1**: nn.Module and nn.Parameter\n",
    "\n",
    "```python\n",
    "weight = torch.nn.Parameter(torch.randn(2, 1), requires_grad=True)\n",
    "bias = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implementing a Neuron ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. The sigmoid function\n",
    "# 2. A neuron using torch.nn.Module but with manual parameter handling\n",
    "# 3. Create a neuron with specified input features and initialize weights\n",
    "# 4. Define an input vector x with two features\n",
    "# 5. Use the neuron to compute an output\n",
    "\n",
    "def sigmoid(x:torch.Tensor) -> torch.Tensor:\n",
    "    # The sigmoid function is defined as:\n",
    "    # sigmoid(x) = 1 / (1 + exp(-x))\n",
    "    return # Add your code\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, n_features:int, activation:callable=sigmoid) -> None:\n",
    "        # Initialize the neuron as a PyTorch module\n",
    "        super().__init__()\n",
    "        # Initialize weights with random values - Parameter makes it learnable\n",
    "        self.weights = # Add your code\n",
    "        \n",
    "        # Initialize bias to a random value\n",
    "        self.bias = # Add your code\n",
    "        \n",
    "        self.activation = # Add your code\n",
    "\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Compute linear output manually (dot product of inputs and weights + bias)\n",
    "        # 2. Apply the activation function to the result\n",
    "        # 3. Return the output\n",
    "        \n",
    "        linear_output = # Add your code\n",
    "        \n",
    "        output = # Add your code\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the neuron\"\"\"\n",
    "        return f\"Neuron (weights = {self.weights.detach()}, bias = {self.bias.detach()})\"\n",
    "\n",
    "# Create a neuron with 2 input features\n",
    "neuron = Perceptron(n_features=2)\n",
    "\n",
    "# Define an input vector x with two features\n",
    "# with values 1.0, 2.0\n",
    "x = # Add your code\n",
    "\n",
    "# Let's use our neuron to compute an output\n",
    "output = neuron(x)  # In PyTorch modules, we call the object directly instead of .forward()\n",
    "\n",
    "# Print the output to see the result\n",
    "print(f\"Output from neuron: {output.item()}\")\n",
    "print(neuron)\n",
    "print(f\"Neuron has {len(neuron.weights)} weights\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'sigmoid': sigmoid,\n",
    "    'neuron_output': output,\n",
    "    'neuron_input_size': len(neuron.weights),\n",
    "    'has_bias': hasattr(neuron, 'bias'),\n",
    "    'is_parameterized': isinstance(neuron.weights, torch.nn.Parameter)\n",
    "}\n",
    "checker.check_exercise(1, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, G, pos = utils.plotting.visualize_network_nx(neuron, figsize=(16, 4))\n",
    "\n",
    "# Display information about the perceptron\n",
    "print(f\"Perceptron Architecture:\")\n",
    "print(f\"  - Input features: {len(neuron.weights)}\")\n",
    "print(f\"  - Activation function: {neuron.activation.__name__ if hasattr(neuron.activation, '__name__') else 'Custom'}\")\n",
    "print(f\"  - Output: 1 (probability between 0 and 1)\")\n",
    "print(f\"\\nGraph Properties:\")\n",
    "print(f\"  - Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  - Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Neurons to Solve Problems\n",
    "***\n",
    "### Case Study: Space Shuttle Challenger Disaster\n",
    "\n",
    "The explosion of the Space Shuttle Challenger on January 28, 1986, resulted from the failure of O-rings sealing the propulsion system. Critical components hadn't been properly tested at low temperatures, leading to catastrophic failure.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/challenger.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "We'll analyze data from the Challenger disaster, examining the relationship between temperature at launch and O-ring failures. This real-world example demonstrates how neural networks can identify crucial patterns that might prevent disasters.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| `n_risky` | Number of O-rings at risk during launch |\n",
    "| `n_distressed` | Number of O-rings experiencing thermal distress |\n",
    "| `temp` | Temperature at launch (in degrees Fahrenheit) |\n",
    "| `leak_psi` | Pressure at which the O-rings leak |\n",
    "| `temporal_order` | Temporal order of the launch |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('challenger',\n",
    "                                           dest_path=data_path,\n",
    "                                           extract=True,\n",
    "                                           remove_compressed=True)\n",
    "dataset_path = dataset_path / 'o-ring-erosion-only.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_risky', 'n_distressed', 'temp', 'leak_psi', 'temporal_order']\n",
    "\n",
    "df = pd.read_csv(dataset_path, sep=\"\\\\s+\", header=None, names=cols)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparing the Data\n",
    "***\n",
    "We'll build a model that predicts launch success based on temperature. Our goal is to use the `temp` variable as a feature to predict the `n_distressed` variable (O-ring failures).\n",
    "\n",
    "### Pre-processing Steps\n",
    "1. Convert temperature from Fahrenheit to Celsius\n",
    "2. Cap the number of distressed O-rings to be between 0 and 1 (binary outcome)\n",
    "3. Normalize the temperature using standardization (z-score)\n",
    "4. Split the data into training and test sets\n",
    "\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Hint**: Our input and target are one-dimensional, but we need to reshape them to be two-dimensional to represent the number of samples and the number of features. Use `.view(-1, 1)` for reshaping in PyTorch.\n",
    "\n",
    "***\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> **Snippet 2**: Splitting data with sklearn\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Data Preparation ðŸŽ¯\n",
    "# Implement:\n",
    "# 1. Convert temperature from Fahrenheit to Celsius\n",
    "# 2. Cap n_distressed values at 1.0 (binary outcome)\n",
    "# 3. Convert features and targets to PyTorch tensors\n",
    "# 4. Normalize the temperature data\n",
    "# 5. Split the data into training and testing sets (80% train, 20% test)\n",
    "\n",
    "# Step 1: Convert temperature from Fahrenheit to Celsius\n",
    "def f_to_c(f:float) -> float:\n",
    "    # The conversion formula is:\n",
    "    # C = (F - 32) * 5 / 9\n",
    "    return # Add your code\n",
    "\n",
    "# Apply the conversion function to the temperature column\n",
    "temp = # Add your code\n",
    "\n",
    "# Step 2: Cap the n_distressed at 1.0 (binary outcome)\n",
    "n_distressed = # Add your code\n",
    "\n",
    "# Step 3: Convert features and targets to PyTorch tensors\n",
    "X = # Add your code\n",
    "y = # Add your code\n",
    "\n",
    "# Step 4: Normalize the temperature data for better training\n",
    "# Use Standardization (z-score normalization)\n",
    "X_mean = # Add your code\n",
    "X_std = # Add your code\n",
    "X_normalized = # Add your code\n",
    "\n",
    "# Step 5: Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = # Add your code\n",
    "# Print the shapes of the training and testing sets\n",
    "print(f\"Training data shape: {X_train.shape} inputs and {y_train.shape} targets\")\n",
    "print(f\"Testing data shape: {X_test.shape} inputs and {y_test.shape} targets\")\n",
    "print(f\"Temperature mean: {X_mean.item():.2f}Â°C, std: {X_std.item():.2f}Â°C\")\n",
    "print(f\"Number of O-ring distress cases: {y.sum().item()} out of {len(y)}\")\n",
    "print(f\"Number of O-ring distress cases: {y.sum().item()} out of {len(y)}\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'f_to_c': f_to_c,\n",
    "    'X_shape': X_normalized.shape,\n",
    "    'y_shape': y.shape,\n",
    "    'X_mean': X_mean.item(),\n",
    "    'X_std': X_std.item(),\n",
    "    'n_distress_cases': y.sum().item(),\n",
    "    'data_clipped': bool(n_distressed.max() <= 1.0),\n",
    "    'train_test_split_ratio': len(X_train) / len(X) >= 0.75 and len(X_train) / len(X) <= 0.85,\n",
    "    'tensors_created': isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor)\n",
    "}\n",
    "\n",
    "checker.check_exercise(2, answer)\n",
    "\n",
    "# Plot the data to visualize the relationship\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.scatter(X.numpy(), y.numpy(), alpha=0.5, edgecolors='k')\n",
    "\n",
    "utils.plotting.make_fig_pretty(ax, title=\"O-Ring Distress vs Temperature\",\n",
    "                             xlabel=\"Temperature (Â°C)\", ylabel=\"Distress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Forward Pass\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: The forward pass is the process of feeding input data through the neural network to get predictions.\n",
    "\n",
    "In our O-ring example:\n",
    "- **Feature**: Temperature (X_normalized) - what our model uses for predictions\n",
    "- **Target**: O-ring distress (y) - what we're trying to predict\n",
    "\n",
    "### Forward Pass Process\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Input data (features) is fed into the network |\n",
    "| 2 | Each layer processes the inputs it receives |\n",
    "| 3 | Activation functions add non-linearity |\n",
    "| 4 | Final layer produces output predictions |\n",
    "\n",
    "For our single neuron model with temperature as input, this is expressed as:\n",
    "\n",
    "$$\\hat{y} = \\sigma(w \\cdot \\text{temperature} + b)$$\n",
    "\n",
    "### 2.2.1 Batch Processing vs. Individual Samples\n",
    "\n",
    "In our implementation, we're processing one sample at a time in our loops:\n",
    "\n",
    "```python\n",
    "for i in range(len(X_normalized)): \n",
    "    predictions[i] = model(X_normalized[i])\n",
    "```\n",
    "\n",
    "This approach works for our small dataset (23 samples) and single feature, but in production machine learning, data is typically processed in batches for efficiency:\n",
    "\n",
    "```python\n",
    "# Process all samples in a single operation\n",
    "predictions = model(X_normalized)  # For batch-compatible models\n",
    "```\n",
    "\n",
    "## 2.3 Loss Functions and Error Measurement\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: A loss function measures how far the predicted output of a model is from the actual target value.\n",
    "\n",
    "### Why Measure Error?\n",
    "\n",
    "Loss functions provide quantitative feedback on model performance:\n",
    "\n",
    "1. **Optimization guidance**: Indicates direction for weight/bias adjustments\n",
    "2. **Performance tracking**: Monitors model improvement during training\n",
    "3. **Model comparison**: Provides objective metrics for different models\n",
    "4. **Convergence detection**: Helps determine when to stop training\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "We'll use the MSE loss function, defined as:\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $ n $ number of samples\n",
    "- $ y_i $ actual target value for sample $ i $\n",
    "- $ \\hat{y}_i $ predicted target value for sample $ i $\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Note**: For binary classification problems, Binary Cross-Entropy (BCE) loss is typically more suitable than MSE, but we're using MSE here for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implementing a Loss Function ðŸŽ¯\n",
    "# In this exercise, you will:\n",
    "# 1. Implement a Mean Squared Error (MSE) loss function\n",
    "# 2. Create a modified neuron for the O-ring data\n",
    "# 3. Compute predictions using the forward pass\n",
    "# 4. Calculate the loss between predictions and actual targets\n",
    "\n",
    "def mse_loss(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Mean Squared Error loss function\n",
    "    \n",
    "    MSE = 1/n * Î£ (predictions - targets)Â²\n",
    "    where n is the number of samples.\n",
    "    \"\"\"\n",
    "    # Calculate the squared difference between predictions and targets\n",
    "    squared_diff = # Add your code\n",
    "    \n",
    "    # Return the mean of the squared differences\n",
    "    return # Add your code\n",
    "\n",
    "# Create a neuron specifically for the temperature data (1 input feature)\n",
    "model = # Add your code\n",
    "print(f\"Initial model: {model}\")\n",
    "\n",
    "# Forward pass: Feed the normalized temperature data through the neuron\n",
    "predictions = torch.zeros_like(y)\n",
    "\n",
    "# Loop through each input sample and compute the prediction\n",
    "for i in tqdm(range(len(X_normalized)), desc=\"Computing predictions\"):\n",
    "    # Use the model to compute the prediction for each input sample\n",
    "    predictions[i] = # Add your code\n",
    "\n",
    "\n",
    "# Calculate the loss using our MSE loss function\n",
    "loss = mse_loss(predictions, y)\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "\n",
    "# Alternatively, we can use PyTorch's built-in MSE loss function\n",
    "pytorch_mse = torch.nn.MSELoss()\n",
    "loss_pytorch = pytorch_mse(predictions, y)\n",
    "print(f\"PyTorch MSE loss: {loss_pytorch.item():.4f}\")\n",
    "\n",
    "# Check if our implementation matches PyTorch's (should be very close)\n",
    "loss_function_correct = torch.isclose(loss, loss_pytorch, rtol=1e-4)\n",
    "print(f\"Our MSE implementation matches PyTorch: {loss_function_correct}\")\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'mse_loss': mse_loss,\n",
    "}\n",
    "checker.check_exercise(3, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Backward Pass (Backpropagation)\n",
    "***\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Definition**: The backward pass computes gradients of the loss with respect to the network parameters, which are used to update those parameters.\n",
    "\n",
    "After computing the loss, we need to adjust our model's parameters (weights and bias) to improve predictions. This is done through **backpropagation**.\n",
    "\n",
    "### Backpropagation Process\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Computing gradients | Calculate how loss changes with respect to each parameter |\n",
    "| 2 | Propagating error backward | Work from output layer back to input layer |\n",
    "| 3 | Applying the chain rule | Break down complex derivatives into simpler parts |\n",
    "\n",
    "### Gradient Descent Update Rule\n",
    "\n",
    "Once we have gradients, we update parameters:\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "$$b_{new} = b_{old} - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ learning rate (controls step size)\n",
    "- $\\frac{\\partial L}{\\partial w}$ gradient of loss with respect to weight\n",
    "- $\\frac{\\partial L}{\\partial b}$ gradient of loss with respect to bias\n",
    "\n",
    "### PyTorch's Autograd System\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> **Important**: PyTorch automatically computes gradients using autograd, simplifying the backpropagation process.\n",
    "\n",
    "When `requires_grad=True` is set on tensors (default for `nn.Parameter`):\n",
    "\n",
    "1. PyTorch **tracks operations** in a computational graph\n",
    "2. When we call `loss.backward()`, PyTorch **computes gradients** for all tracked tensors\n",
    "3. Gradients are **stored in the `.grad` attribute** of each tensor\n",
    "4. Gradients should be **zeroed out** before each training iteration to avoid accumulation from previous iterations. For the parameters, we can use `parameter.grad.zero_()` to reset gradients.\n",
    "\n",
    "### Neural Network Training Cycle\n",
    "\n",
    "| Phase | Description |\n",
    "|-------|-------------|\n",
    "| Forward Pass | Compute predictions with current parameters |\n",
    "| Loss Calculation | Measure error between predictions and true values |\n",
    "| Backward Pass | Compute gradients of loss with respect to parameters |\n",
    "| Parameter Update | Adjust parameters to reduce loss |\n",
    "\n",
    "This cycle repeats for multiple epochs until the model converges to a solution.\n",
    "\n",
    "Now let's implement the backward pass and parameter updates for our O-ring prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Implementing the Backward Pass and Parameter Updates ðŸŽ¯\n",
    "# In this exercise, you will:\n",
    "# 1. Implement a training loop with forward and backward passes\n",
    "# 2. Use PyTorch's autograd to compute gradients\n",
    "# 3. Update the model parameters using gradient descent\n",
    "# 4. Visualize how the model's predictions improve over time\n",
    "\n",
    "# Reset our model for training\n",
    "model = Perceptron(n_features=1)\n",
    "print(f\"Initial model parameters: {model}\")\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = # Add your code\n",
    "num_epochs = # Add your code\n",
    "\n",
    "# Lists to store training progress\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [Time: {elapsed}<{remaining}]\"):\n",
    "    # Forward pass: compute predictions\n",
    "    predictions = torch.zeros_like(y)\n",
    "    for i in range(len(X_normalized)):\n",
    "        predictions[i] = # Add your code\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = # Add your code\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    # Add your code\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    with torch.no_grad():  # No need to track gradients during the update\n",
    "        # Your code here: Update weights using gradient descent\n",
    "        model.weights -= # Add your code\n",
    "        # Your code here: Update bias using gradient descent\n",
    "        model.bias -= # Add your code\n",
    "    \n",
    "    # Zero gradients for the next iteration\n",
    "    # Your code here: Zero out the gradients\n",
    "    # Add your code\n",
    "    # Add your code\n",
    "\n",
    "print(f\"Final model parameters: {model}\")\n",
    "print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "# Calculate final predictions\n",
    "with torch.no_grad():\n",
    "    final_predictions = torch.zeros_like(y)\n",
    "    for i in range(len(X_normalized)):\n",
    "        final_predictions[i] = model(X_normalized[i])\n",
    "\n",
    "# âœ… Check your answer - is the loss decreasing over time?\n",
    "answer = {\n",
    "    'training_convergence': train_losses[0] > train_losses[-1],\n",
    "    'final_loss': train_losses[-1],\n",
    "    'negative_weight': model.weights.item() < 0\n",
    "}\n",
    "checker.check_exercise(4, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot training loss\n",
    "utils.plotting.plot_loss(\n",
    "    train_losses, \n",
    "    title=\"Training Loss Over Time\",\n",
    "    tufte_style=True,\n",
    "    title_fsize=18,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "# Plot model predictions\n",
    "utils.plotting.plot_model_predictions_SE02(\n",
    "    model=model,\n",
    "    X=X_normalized,\n",
    "    y=y,\n",
    "    X_mean=X_mean.item(),\n",
    "    X_std=X_std.item(),\n",
    "    special_temp=challenger_temp,\n",
    "    special_temp_label=f'Challenger Launch: {challenger_temp:.1f}Â°C',\n",
    "    title=\"O-Ring Failure Probability vs Temperature\",\n",
    "    tufte_style=True,\n",
    "    ax=ax2,\n",
    "    title_fsize=18,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization of the O-ring failure model\n",
    "# This uses our external interactive visualization function\n",
    "\n",
    "# Create the interactive visualization widget\n",
    "main_ui, out = utils.plotting.create_interactive_neuron_visualizer(\n",
    "    X=X_normalized,\n",
    "    y=y,\n",
    "    X_mean=X_mean.item(),\n",
    "    X_std=X_std.item(),\n",
    "    challenger_temp=challenger_temp,\n",
    "    neuron_class=Perceptron,\n",
    "    loss_function=mse_loss,\n",
    "    initial_weight=float(model.weights.item()),\n",
    "    initial_bias=float(model.bias.item()),\n",
    "    learning_rate=0.1,\n",
    "    training_epochs=100\n",
    ")\n",
    "\n",
    "# Display the interactive widget\n",
    "display(main_ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Using the model to predict O-ring distress\n",
    "***\n",
    "Having trained out model, we now want to use it to predict the potential for O-ring distress at different temperatures. Using models in this way is called *inference*. To do this, we need to consider the following:\n",
    "\n",
    "- Data preparation: The data we want to predict on needs to be in the same format as the data we trained on. This means that we need to apply the same transformations (e.g., normalization) to the new data.\n",
    "- Gradient tracking: When we are using the model for inference, we do not need to track gradients. This is because we are not updating the model parameters during inference. We can use `torch.no_grad()` to disable gradient tracking.\n",
    "- Model input: The model input should be a tensor with the same shape as the training data. If we are predicting on a single sample, we need to add an extra dimension to the tensor to make it 2D.\n",
    "\n",
    "Let's implement the inference process for our O-ring prediction model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Making Predictions for the Challenger Launch ðŸŽ¯\n",
    "# In this exercise, you will make predictions about the O-ring failure at the Challenger launch temperature\n",
    "# and calculate the relative risk compared to normal temperatures\n",
    "\n",
    "# The Challenger launch temperature was 31Â°F (-0.56Â°C)\n",
    "challenger_launch_temp = f_to_c(31.0)\n",
    "print(f\"Challenger launch temperature: {challenger_launch_temp:.2f}Â°C\")\n",
    "\n",
    "# Normalize the temperature using the same parameters as our training data\n",
    "challenger_temp_normalized = # Add your code\n",
    "challenger_tensor = # Add your code\n",
    "\n",
    "# Use the trained model to predict the probability of O-ring failure\n",
    "with torch.no_grad():\n",
    "    failure_probability = model(challenger_tensor).item()\n",
    "\n",
    "print(f\"Predicted probability of O-ring failure: {failure_probability:.4f} or {failure_probability*100:.1f}%\")\n",
    "\n",
    "# Determine if this probability indicates a high risk\n",
    "# A common threshold in binary classification is 0.5 (50% probability)\n",
    "failure_risk = \"HIGH RISK\" if failure_probability > 0.5 else \"LOW RISK\"\n",
    "\n",
    "# What is your launch recommendation based on this analysis?\n",
    "launch_recommendation = \"DO NOT LAUNCH\" if failure_probability > 0.5 else \"PROCEED WITH LAUNCH\"\n",
    "\n",
    "print(f\"Risk assessment: {failure_risk}\")\n",
    "print(f\"Launch recommendation: {launch_recommendation}\")\n",
    "\n",
    "# Calculate how many times more likely failure is at the launch temperature compared to a warmer day (20Â°C)\n",
    "warm_temp = 20.0  # A much warmer temperature in Celsius\n",
    "warm_temp_normalized = (warm_temp - X_mean.item()) / X_std.item()\n",
    "warm_tensor = torch.tensor([warm_temp_normalized], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    warm_failure_probability = model(warm_tensor).item()\n",
    "\n",
    "relative_risk = failure_probability / warm_failure_probability if warm_failure_probability > 0 else float('inf')\n",
    "print('-' * 80)\n",
    "print(f\"Probability of failure at 20Â°C: {warm_failure_probability:.4f} or {warm_failure_probability*100:.1f}%\")\n",
    "print(f\"Failure at launch temperature is {relative_risk:.1f}x more likely than at 20Â°C\")\n",
    "\n",
    "\n",
    "# âœ… Check your answer\n",
    "answer = {\n",
    "    'challenger_failure_prob': failure_probability,\n",
    "    'recommendation': launch_recommendation == \"DO NOT LAUNCH\",\n",
    "    'relative_risk_factor': relative_risk > 3,  # Is risk at least 3 times higher?\n",
    "}\n",
    "checker.check_exercise(5, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Key Insights from Our Analysis\n",
    "***\n",
    "Our trained model shows a strong correlation between temperature and O-ring failure probability. At the Challenger launch temperature of 31Â°F (-0.56Â°C), the model predicts an extremely high probability of failure.\n",
    "\n",
    "### Critical Lessons\n",
    "\n",
    "| Insight | Description |\n",
    "|---------|-------------|\n",
    "| **Temperature threshold** | Risk increases dramatically below a critical temperature point |\n",
    "| **Data-driven decisions** | Statistical analysis could have provided strong evidence against launching |\n",
    "| **Risk assessment** | The model correctly identifies risk despite limited low-temperature data |\n",
    "| **Communication** | Technical findings must be effectively communicated to decision-makers |\n",
    "\n",
    "The Rogers Commission, which investigated the disaster, found that NASA managers had disregarded warnings from engineers about the dangers of launching in cold temperaturesâ€”a tragic example of how crucial effective communication of technical risks can be.\n",
    "\n",
    "### 2.6.1 The Challenger Disaster and Data Science Ethics\n",
    "***\n",
    "Our analysis demonstrates a sobering point: proper data analysis could have potentially prevented this disaster. As our model shows, the probability of O-ring failure increases dramatically at lower temperatures, and the launch temperature of 31Â°F (-0.56Â°C) was well within the danger zone.\n",
    "\n",
    "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/reminder.svg\" width=\"20\"/> **Ethics Note**: When lives depend on our analyses, data scientists have a responsibility to be thorough, transparent, and to clearly communicate risks to decision-makers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
